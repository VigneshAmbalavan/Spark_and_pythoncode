*****finding userid in add to cart mode and click under electronic & fashion , but that user should not be in both electrnic & fashion*******
***************make a file in which columns will be as userid,categories,action(no primary key is involved)*****************
hadoop fs -cat retailanalysis/retailcart.csv
******external table created with header skip properties*****************************
create table userdata1(
     userid Int,
     category String,
     action string)
     row format delimited 
     fields terminated by ','
     tblproperties("skip.header.line.count"="1");

      load  data inpath '/user/cloudera/retailanalysis' overwrite into table userdata1;
      insert into table userdata1 values(005,"fashion","addtocart");

      *****problem--not able to see the newly inserted data in hive**********************

****************MSCk repair table tablename---------> is the command for Fix partition addition*******

userid,category,action   ***************using spark using RDD api *************
001,fashion,addtocart      **firstly filter it on categories and action******************
002,electronics,purchased      **001,fashion,addtocart**
003,cosmetics,click            **004,electronics,addtocart**
004,electronics,addtocart      **004,electronics,click**
001,electronics,click          **003,electronics,addtocart**
002,cosmetics,addtocart        **002,fashion,addtocart**
003,electronics,addtocart      **001,electronics,click**
004,cosmetics,click            
001,cosmetics,purchased
002,fashion,addtocart
003,fashion,purchased
004,fashion,purchased
001,electronics,click
002,fashion,addtocart

select * from userdata1 where category in("electronics","fashion");


create table userdata_fashion(
userid Int,
category string,
action string);

insert overwrite table userdata_fashion select * from userdata1 where category = 'fashion' and action in ("addtocart","click");

1	fashion	addtocart
2	fashion	addtocart
2	fashion	addtocart


create table userdata_electronics(
userid Int,
category string,
action string);

insert overwrite  table userdata_electronics select * from userdata1 where category = 'electronics' and action in ("addtocart","click");

4	electronics	addtocart
1	electronics	click
3	electronics	addtocart
1	electronics	click

create table temp (
userid int);

insert overwrite table temp select a.userid from userdata_fashion a join userdata_electronics b on a.userid=b.userid;

MSCK REPAIR TABLE userdata1;

REFRESH [retail_db.]userdata1

*************final-result(finding details of user which have action tag is addtocart or click under category fashion and electronics but that user should not present in both electronics and fashion)**********

select * from userdata1 as x where x.action in('addtocart','click') and x.category in('fashion','electronics') and x.userid != (select y.userid from temp as y);********************not working

 select * from userdata1 where action in('addtocart','click') and category in('fashion','electronics') and userid != (select userid from temp);********not working

 select * from userdata1 as a where a.action in('addtocart','click') and a.category in('fashion','electronics') and a.userid not in (select userid from temp)************working but seems not optimal in resource utilization*************************
******result verified it is correct*************************************
4	electronics	addtocart
3	electronics	addtocart
2	fashion	addtocart
2	fashion	addtocart
*******************************************************************************

***************using spark using RDD api *************
val data= sc.textFile("/user/cloudera/retailanalysis").mapPartitionsWithIndex((idx,itr)=>(if(idx==0) itr.drop(1) else itr)).map(x=>x.split(","))
val data_fashion = data.filter(x=>(x(1)=="fashion")).map(x=>(x(0),x))
val data_electronics = data.filter(x=>(x(1)=="electronics")).map(x=>(x(0),x))
val data_intersection = data_fashion.intersection(data_electronics)
val data_union = data_fashion.union(data_electronics)
val final_result = data_union.substract(data_intersection)
********the above code need fix**********************************************
**************experiments*********************
val data1 = sc.parallelize(Seq(("a", 1), ("a", 2), ("b", 2), ("b", 3), ("c", 1)))
val data2 = sc.parallelize(Seq(("a", 3), ("b", 5)))
// broadcast data2 key list to use in filter method, which runs in executor nodes*************************************
val bcast = sc.broadcast(data2.map(_._1).collect())********************====>******x=>x._1********************
val result = data1.filter(r => bcast.value.contains(r._1))
println(result.collect().toList)
*******************************************************************************************************
********creating textfie into dataframe directly*************************8
val conf = new SparkConf().setAppName(appName).setMaster("local")
val sc = SparkContext(conf)
********************************// raw text file*******************************************************
val file = sc.textFile("C:\\vikas\\spark\\Interview\\text.txt")
val fileToDf = file.map(_.split(",")).map{case Array(a,b,c) => 
(a,b.toInt,c)}.toDF("name","age","city")
fileToDf.foreach(println(_))
*************************************// spark session without schema***********************************************************
import org.apache.spark.sql.SparkSession
val sparkSess = 
SparkSession.builder().appName("SparkSessionZipsExample")
.config(conf).getOrCreate()
val df = sparkSess.read.option("header", 
"false").csv("C:\\vikas\\spark\\Interview\\text.txt")
df.show()
*********************************//Spark session with schema*********************************************************************
import org.apache.spark.sql.types._
val schemaString = "name age city"
val fields = schemaString.split(" ").map(fieldName => StructField(fieldName, 
StringType, nullable=true))
val schema = StructType(fields)
val dfWithSchema = sparkSess.read.option("header", 
"false").schema(schema).csv("C:\\vikas\\spark\\Interview\\text.txt")
dfWithSchema.show()
*************************************// using sql context**************************************************************************
org.apache.spark.sql.SQLContext
val fileRdd = 
sc.textFile("C:\\vikas\\spark\\Interview\\text.txt").map(_.split(",")).map{x 
=> org.apache.spark.sql.Row(x:_*)}
val sqlDf = sqlCtx.createDataFrame(fileRdd,schema)
sqlDf.show()
*********************EXadatum Expected question solution**************************************************
create table weblog
customer_id,Page_url,visit_ts
001,https://facebook.com,20170109010000
002,"https://facebook.com",20170109010200
003,"https://google.com",20170109021656
001,"https://bigbasket.com",20170109020212
004,"https://facebook.com",20170109020909  
004,"https://myntra.com",20170109030202
003,"https://facebook.com",20170109030909
002,"https://myntra.com",20170109030910
005,"https://facebook.com",20170109040917
004,"https://ola.com",20170109040117
*****script to load weblog.csv into its hive table every hour************************************



















